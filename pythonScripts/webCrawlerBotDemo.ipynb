{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def organize_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if os.path.isfile(filename):\n",
    "            file_extension = os.path.splitext(filename)[1]\n",
    "            if not os.path.exists(file_extension):\n",
    "                os.makedirs(file_extension)\n",
    "            shutil.move(filename, os.path.join(file_extension, filename))\n",
    "# Replace 'path_to_directory' with the directory you want to organize\n",
    "organize_files(r\"C:\\Users\\srira\\OneDrive\\Desktop\\GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\srira\\appdata\\roaming\\python\\python312\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\srira\\appdata\\roaming\\python\\python312\\site-packages (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\srira\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\srira\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\srira\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srira\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\srira\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4) (2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_crawler(url): \n",
    "    \n",
    "    # Send HTTP request to the URL \n",
    "\n",
    "    response = requests.get(url) \n",
    "\n",
    "    # Check if the request was successful (status code 200) \n",
    "\n",
    "    if response.status_code == 200: \n",
    "\n",
    "        # Parse HTML content with BeautifulSoup \n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser') \n",
    "\n",
    "        # Extract and print relevant information (modify as needed) \n",
    "\n",
    "        title = soup.title.text \n",
    "\n",
    "        print(f'Title: {title}') \n",
    "        paragraphs = soup.find_all('p')\n",
    "        for i, para in enumerate(paragraphs):\n",
    "            print(f'Paragraph {i + 1}: {para.get_text()}')\n",
    "\n",
    "        # Additional data extraction and processing can be added here \n",
    "        # Extract all headers (h1, h2, etc.)\n",
    "        for header_tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            headers = soup.find_all(header_tag)\n",
    "            for header in headers:\n",
    "                print(f'{header_tag.upper()} Found: {header.get_text()}')\n",
    "        \n",
    "         # Extract all image sources (img tags)\n",
    "        images = soup.find_all('img')\n",
    "        for i, img in enumerate(images):\n",
    "            img_url = img.get('src')\n",
    "            print(f'Image {i + 1}: {img_url}')\n",
    "        \n",
    "            # Optionally download the image\n",
    "            download_image(img_url, f'image_{i + 1}.jpg')\n",
    "            \n",
    "    else: \n",
    "\n",
    "        print(f'Error: Failed to fetch {url}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def simple_crawler(url, save_dir='images'): \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Send HTTP request to the URL \n",
    "    response = requests.get(url) \n",
    "\n",
    "    # Check if the request was successful (status code 200) \n",
    "    if response.status_code == 200: \n",
    "        # Parse HTML content with BeautifulSoup \n",
    "        soup = BeautifulSoup(response.text, 'html.parser') \n",
    "\n",
    "        # Extract and print the title \n",
    "        title = soup.title.text if soup.title else \"No Title\"\n",
    "        print(f'Title: {title}') \n",
    "\n",
    "        # Extract all paragraphs \n",
    "        paragraphs = soup.find_all('p')\n",
    "        for i, para in enumerate(paragraphs):\n",
    "            print(f'Paragraph {i + 1}: {para.get_text()}')\n",
    "\n",
    "        # Extract all headers (h1, h2, etc.)\n",
    "        for header_tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            headers = soup.find_all(header_tag)\n",
    "            for header in headers:\n",
    "                print(f'{header_tag.upper()} Found: {header.get_text()}')\n",
    "\n",
    "        # Extract all links (a tags)\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for i, link in enumerate(links):\n",
    "            print(f'Link {i + 1}: {link.get(\"href\")} Text: {link.get_text()}')\n",
    "\n",
    "        # Extract all image sources (img tags)\n",
    "        images = soup.find_all('img')\n",
    "        for i, img in enumerate(images):\n",
    "            img_url = img.get('src')\n",
    "            print(f'Image {i + 1}: {img_url}')\n",
    "        \n",
    "            # Optionally download the image\n",
    "            download_image(img_url, f'{save_dir}/image_{i + 1}.jpg')\n",
    "\n",
    "    else: \n",
    "        print(f'Error: Failed to fetch {url}') \n",
    "\n",
    "def download_image(img_url, filename):\n",
    "    # Check if the image URL is valid\n",
    "    if not img_url.startswith(('http://', 'https://')):\n",
    "        img_url = f\"http:{img_url}\"  # Handle relative URLs\n",
    "    try:\n",
    "        img_response = requests.get(img_url, stream=True)\n",
    "        if img_response.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                for chunk in img_response:\n",
    "                    f.write(chunk)\n",
    "            print(f'Successfully downloaded {filename}')\n",
    "        else:\n",
    "            print(f'Failed to download image {img_url}')\n",
    "    except Exception as e:\n",
    "        print(f'Error downloading image: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Prachodayan\n",
      "Link 1: index.html Text: \n",
      "Link 2: Courses.html Text: Courses\n",
      "Link 3: Research.html Text: Research\n",
      "Link 4: AboutUs.html Text: About Us\n",
      "Link 5: QC_Home.html Text: Quantum Computing\n",
      "Link 6: https://qiskit.org/textbook/preface.html Text: https://qiskit.org/textbook/preface.html.\n",
      "Image 1: home.jpeg\n",
      "Error downloading image: Invalid URL 'http:home.jpeg': No host supplied\n",
      "Image 2: qc.png\n",
      "Error downloading image: Invalid URL 'http:qc.png': No host supplied\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "simple_crawler('https://prachodayan.com/QC_Introduction.html', save_dir=r'C:\\Users\\srira\\OneDrive\\Pictures')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
